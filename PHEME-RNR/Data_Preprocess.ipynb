{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6a7a8d",
   "metadata": {},
   "source": [
    "### Data Format Conversion: json > csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c55ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62b14e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter irrelevant characters and links\n",
    "def sentence_process(string):\n",
    "    \n",
    "    word_list = string.split(' ')\n",
    "    new_list = []\n",
    "    for word in word_list:\n",
    "        if word.startswith('@') or ('http' in word) or ('@' in word):\n",
    "            continue\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "    final_string = ' '.join(new_list)\n",
    "    if not final_string.endswith('\\n'):\n",
    "        final_string += '\\n'\n",
    "\n",
    "    # Remove emojis using regex\n",
    "    final_string = re.sub(r'[^\\w\\s,.\\-?!\\'\"]', '', final_string)\n",
    "    \n",
    "    return final_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b47b6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = ['text_comments','text_only','comments_only','label','count']\n",
    "csv_list = []\n",
    "# event_list = ['charliehebdo','ferguson','germanwings-crash','ottawashooting','sydneysiege']\n",
    "event_list = ['bostonbombings']\n",
    "# base_path ='./data/pheme-rnr'\n",
    "# base_path ='./TwitterDataset/phemernrdataset/pheme-rnr-dataset/'\n",
    "base_path ='/home/angelos/MyThesis/PHEME-RNR/TwitterDataset/phemernrdataset/pheme-rnr-dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98baf95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in event_list:\n",
    "    news_path = os.path.join(base_path, event)\n",
    "    rumour_path = os.path.join(news_path,'rumours')\n",
    "    nonrumour_path = os.path.join(news_path,'non-rumours')\n",
    "\n",
    "    for rumour_tweet in os.listdir((rumour_path)):\n",
    "        full_path = os.path.join(rumour_path, rumour_tweet)\n",
    "        if rumour_tweet.startswith(\"._\") or not os.path.isdir(full_path):\n",
    "            continue\n",
    "            \n",
    "        text_comments = \"\"\n",
    "        text_only = \"\"\n",
    "        comments_only = \"\"\n",
    "        count = 0\n",
    "        \n",
    "        rumour_text = os.path.join(rumour_path, rumour_tweet, 'source-tweets')\n",
    "        for tweet_text in os.listdir(rumour_text):\n",
    "            if tweet_text.startswith(\"._\") or not tweet_text.endswith(\".json\"):\n",
    "                continue  # skip metadata or invalid files\n",
    "            json_path = os.path.join(rumour_text,tweet_text)\n",
    "            with open(json_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                text_comments += sentence_process(json_data['text'])\n",
    "                text_comments += '[SEP]'\n",
    "                text_only += sentence_process(json_data['text'])\n",
    "                text_only += '[SEP]'\n",
    "\n",
    "        rumour_comment = os.path.join(rumour_path, rumour_tweet, 'reactions')\n",
    "        for comment in os.listdir(rumour_comment):\n",
    "            if comment.startswith(\"._\") or not comment.endswith(\".json\"):\n",
    "                continue  # skip metadata or invalid files\n",
    "            json_path = os.path.join(rumour_comment, comment)\n",
    "            with open(json_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                text = json_data.get('full_text') or json_data.get('text')\n",
    "                if text and sentence_process(text) != '\\n':\n",
    "                    text_comments += sentence_process(text)\n",
    "                    text_comments += '[SEP]'\n",
    "                    comments_only += sentence_process(text)\n",
    "                    comments_only += '[SEP]'\n",
    "                    count += 1\n",
    "\n",
    "        single_list = [text_comments, text_only, comments_only, 'rumour', count]\n",
    "        csv_list.append(single_list)\n",
    "\n",
    "            \n",
    "    for nonrumour_tweet in os.listdir((nonrumour_path)):\n",
    "        full_path = os.path.join(nonrumour_path, nonrumour_tweet)\n",
    "        if nonrumour_tweet.startswith(\"._\") or not os.path.isdir(full_path):\n",
    "            continue\n",
    "            \n",
    "        text_comments = \"\"\n",
    "        text_only = \"\"\n",
    "        comments_only = \"\"\n",
    "        count = 0\n",
    "\n",
    "        nonrumour_text = os.path.join(nonrumour_path, nonrumour_tweet, 'source-tweets')\n",
    "        for tweet_text in os.listdir(nonrumour_text):\n",
    "            if tweet_text.startswith(\"._\") or not tweet_text.endswith(\".json\"):\n",
    "                continue  # skip metadata or invalid files\n",
    "            json_path = os.path.join(nonrumour_text,tweet_text)\n",
    "            with open(json_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                text_comments += sentence_process(json_data['text'])\n",
    "                text_comments += '[SEP]'\n",
    "                text_only += sentence_process(json_data['text'])\n",
    "                text_only += '[SEP]'\n",
    "\n",
    "        nonrumour_comment = os.path.join(nonrumour_path, nonrumour_tweet, 'reactions')\n",
    "        for comment in os.listdir(nonrumour_comment):\n",
    "            if comment.startswith(\"._\") or not comment.endswith(\".json\"):\n",
    "                continue  # skip metadata or invalid files\n",
    "            json_path = os.path.join(nonrumour_comment, comment)\n",
    "            with open(json_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                text = json_data.get('full_text') or json_data.get('text')\n",
    "                if text and sentence_process(text) != '\\n':\n",
    "                    text_comments += sentence_process(text)\n",
    "                    text_comments += '[SEP]'\n",
    "                    comments_only += sentence_process(text)\n",
    "                    comments_only += '[SEP]'\n",
    "                    count += 1\n",
    "\n",
    "        single_list = [text_comments,text_only, comments_only, 'nonrumour',count]\n",
    "        csv_list.append(single_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "957622f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file=pd.DataFrame(columns=title,data=csv_list)\n",
    "csv_file.to_csv('./data/data_for_testing.csv',index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e9673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
